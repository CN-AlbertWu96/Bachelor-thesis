{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import math\n",
    "import heapq \n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from spherecluster import SphericalKMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用iter读数据，确保内存不会炸\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield (line.lower().split())\n",
    "\n",
    "            \n",
    "# 计算词组与文本的tf-idf，输入phrase和所有文本，返回和每个文本的tf-idf\n",
    "def tf_idf(phrase,docs):\n",
    "    phrase_exist = []\n",
    "    for doc in docs:\n",
    "        phrase = phrase.lower().strip()\n",
    "        doc = doc.lower().strip()\n",
    "        tf = doc.count(phrase)/(1+len(doc.split()))\n",
    "        phrase_exist.append(tf)\n",
    "    exist_num = len(phrase_exist)-phrase_exist.count(0)\n",
    "#     print ('-'*30)\n",
    "#     print ('Exist num: %s'%exist_num)\n",
    "    idf = math.log(len(docs)/(1+exist_num))\n",
    "#     print ('Idf: %s'%idf)\n",
    "    return np.array([list(map(lambda i: i*idf, phrase_exist))])\n",
    "\n",
    "# 替换原文本中所有话题为a_b\n",
    "def replace_all(text, phrases):\n",
    "    count = 0\n",
    "    for phrase in phrases:\n",
    "        if count % 100 == 0:\n",
    "            print (count, datetime.now())\n",
    "        text = text.replace(phrase, phrase.replace(' ','_'))\n",
    "        count += 1\n",
    "    return text\n",
    "\n",
    "# 训练，更新话题向量\n",
    "def local_embedding(phrases, docs):\n",
    "    path = 'dataset/%s/%s/temp/docs.txt'%(dataset,level)\n",
    "    if not os.path.isfile(path):\n",
    "        print ('Docs not exists, need to generate docs first.')\n",
    "        with open(path, 'w') as f_write:\n",
    "            for doc in docs:\n",
    "                for sentence in re.split(r'[.?!]', doc):\n",
    "                    sentence_new = re.sub(\",!@#$%^&*()\", \" \",sentence)  \n",
    "                    f_write.write(sentence_new.strip()+'\\n')\n",
    "        f_write.close()\n",
    "        \n",
    "#     sys.exit()\n",
    "    # 训练词向量\n",
    "    # logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # 加载语料\n",
    "    model_path = 'dataset/%s/%s/model'%(dataset,level)\n",
    "    sentences = MyCorpus(path)\n",
    "    # 训练skip-gram模型; 设定维度为100，窗口5，词最小出现次数3，4核并行\n",
    "    if not os.path.isfile(model_path):\n",
    "        print ('Train model')\n",
    "        model = word2vec.Word2Vec(sentences, size=size, window=5, min_count=2, workers=4)\n",
    "        model.save(model_path)\n",
    "    else:\n",
    "        print ('Load model')\n",
    "        model = word2vec.Word2Vec.load(model_path)\n",
    "    # 将话题集合中的每一个话题求phrase embedding\n",
    "    phrases_vector = {}\n",
    "    total_count = 0\n",
    "    none_zero_count = 0\n",
    "    zero_count = 0\n",
    "    for phrase in phrases:\n",
    "        phrase = phrase.replace(' ','_')\n",
    "        vector = np.zeros((size,), dtype=float)\n",
    "        total_count += 1\n",
    "        try:\n",
    "            vector += model.wv[phrase]\n",
    "            none_zero_count += 1\n",
    "            if phrase not in phrases_vector:\n",
    "                phrases_vector[phrase] = vector\n",
    "        # 对于词典中不存在的词，直接取0\n",
    "        except Exception as e:\n",
    "            zero_count += 1\n",
    "    return phrases_vector\n",
    "    \n",
    "def spherical_kmeans(phrases_vector, k):\n",
    "    phrase_list = []\n",
    "    vector_list = []\n",
    "    for phrase in phrases_vector:\n",
    "        vec = phrases_vector[phrase]\n",
    "        phrase_list.append(phrase)\n",
    "        vector_list.append(vec)\n",
    "    X = np.array(vector_list)\n",
    "    \n",
    "    # 假设k=10，用spherical k-means进行聚类\n",
    "    # Number of clusters\n",
    "    skm = SphericalKMeans(n_clusters=k)\n",
    "    # Fitting the input data\n",
    "    kmeans = skm.fit(X)\n",
    "    # Getting the cluster labels\n",
    "    labels = skm.labels_\n",
    "    # Centroid values\n",
    "    centroids = skm.cluster_centers_\n",
    "    \n",
    "    # 获得每个话题的聚类结果\n",
    "    phrases_labels = pd.DataFrame(columns=['label','phrase'])\n",
    "    for i in range(len(phrases_vector)):\n",
    "        phrases_labels = phrases_labels.append({'label': labels[i], 'phrase': phrase_list[i]}, ignore_index=True)\n",
    "    \n",
    "    print ('Write phrases_clustering File......')\n",
    "    phrases_labels.to_csv('dataset/%s/%s/%s/temp/phrases_clustering.csv'%(dataset,model_name,level),index=False)\n",
    "    print ('Completed')\n",
    "    return phrases_labels\n",
    "    \n",
    "def kmeans(phrases_vector, k):\n",
    "    phrase_list = []\n",
    "    vector_list = []\n",
    "    for phrase in phrases_vector:\n",
    "        vec = phrases_vector[phrase]\n",
    "        phrase_list.append(phrase)\n",
    "        vector_list.append(vec)\n",
    "    X = np.array(vector_list)\n",
    "    # 假设k=10，用k-means进行聚类\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    # Fitting the input data\n",
    "    kmeans = kmeans.fit(X)\n",
    "    # Getting the cluster labels\n",
    "    labels = kmeans.predict(X)\n",
    "    # Centroid values\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # 获得每个话题的聚类结果\n",
    "    phrases_labels = pd.DataFrame(columns=['label','phrase'])\n",
    "    for i in range(len(phrases_vector)):\n",
    "        phrases_labels = phrases_labels.append({'label': labels[i], 'phrase': phrase_list[i]}, ignore_index=True)\n",
    "    \n",
    "    print ('Write phrases_clustering File......')\n",
    "    phrases_labels.to_csv('dataset/%s/%s/%s/temp/phrases_clustering.csv'%(dataset,model_name,level),index=False)\n",
    "    print ('Completed')\n",
    "    return phrases_labels\n",
    "    \n",
    "    \n",
    "def extract_docs_by_phrases(phrases_labesl, docs, k):\n",
    "    model_path = 'dataset/%s/%s/model'%(dataset,level)\n",
    "    model = word2vec.Word2Vec.load(model_path)\n",
    "    top_tfidf = 1000\n",
    "    # 利用tf-idf找到与phrase最相关的documents集合，并完成对于每个label下所有documents集合\n",
    "    docs_cluster = []\n",
    "#     for label in range(1):\n",
    "    for label in range(k):\n",
    "        print ('Label: %s'%label)\n",
    "        phrases = phrases_labesl.loc[phrases_labesl['label']==label]['phrase'].tolist()\n",
    "\n",
    "        # 计算每个词组对文档的tf-idf\n",
    "#         for i in range(5):\n",
    "        for i in range(len(phrases)):\n",
    "            phrase = phrases[i].replace(' ','_')\n",
    "            result = tf_idf(phrase,docs)\n",
    "            if i == 0 or i == len(phrases)-1:\n",
    "                print (phrase)\n",
    "                print (result.shape, np.sum(result, axis=1))\n",
    "            if i == 0:\n",
    "                tf_idf_distribution = result\n",
    "            else:\n",
    "                tf_idf_distribution = np.insert(tf_idf_distribution, tf_idf_distribution.shape[0], values=result, axis=0)\n",
    "        print ('Num of phrases: %s'%tf_idf_distribution.shape[0])\n",
    "#         print ('Dim of vector: %s'%tf_idf_distribution.shape[1]) \n",
    "        '''\n",
    "        # 取出所有词中tfidf top1000的documents下标\n",
    "        docs_index = []\n",
    "        for i in range(tf_idf_distribution.shape[0]):\n",
    "            a = tf_idf_distribution[i]\n",
    "            top_index = heapq.nlargest(min(top_tfidf,len(tf_idf_distribution[i])), range(len(a)), a.take)\n",
    "            if top_index[min(top_tfidf,len(top_index)-1)] <= 0:\n",
    "                # 取出所有tfidf不为0的document\n",
    "                top_tfidf = np.count_nonzero(a)\n",
    "                top_index = heapq.nlargest(top_tfidf, range(len(a)), a.take)\n",
    "            docs_index = list(set(docs_index).union((set(top_index))))\n",
    "        print ('Num of docs: %s'%len(docs_index))\n",
    "        \n",
    "        # 生成第i个聚类下的di\n",
    "        docs_i = ''\n",
    "        for i in range(len(docs_index)):\n",
    "            docs_i += docs[docs_index[i]]\n",
    "            docs_i += '$$$'\n",
    "        docs_i = docs_i.strip()\n",
    "        docs_cluster.append((label,docs_i))\n",
    "        print ('-'*20)\n",
    "        '''\n",
    "        \n",
    "        print ('计算平均词向量%s'%datetime.now())\n",
    "        # 计算第k类话题的平均词向量\n",
    "        phrases_embedding = np.zeros((0,size,), dtype=np.float32)\n",
    "        for phrase in phrases:\n",
    "            phrases_embedding = np.insert(phrases_embedding,phrases_embedding.shape[0],values=model.wv[phrase],axis=0)\n",
    "        mean_phrase_embedding = np.mean(phrases_embedding,axis=0)\n",
    "#         print (phrases,mean_phrase_embedding[:3])\n",
    "        \n",
    "        print ('计算文档向量%s'%datetime.now())\n",
    "        # 计算文档向量，利用tf_idf加权的term embedding,并计算和该平均词向量相近的top1000个文本向量对应的文本\n",
    "        phrase_doc_cos_dist = np.array([])\n",
    "        for i in range(tf_idf_distribution.shape[1]):\n",
    "            doc = docs[i]\n",
    "            doc_embedding = np.zeros((size,), dtype=float)\n",
    "            for j in range(tf_idf_distribution.shape[0]):\n",
    "                phrase = phrases[j]\n",
    "                phrase_embedding = model.wv[phrase]\n",
    "                doc_embedding += phrase_embedding*tf_idf_distribution[j,i]\n",
    "            if np.sum(tf_idf_distribution,axis=0)[i] != 0:\n",
    "                doc_embedding = doc_embedding/np.sum(tf_idf_distribution,axis=0)[i]\n",
    "                dist = 1-sp.spatial.distance.cosine(mean_phrase_embedding, doc_embedding)\n",
    "            else:\n",
    "                doc_embedding = doc_embedding/1\n",
    "                dist = 0\n",
    "            phrase_doc_cos_dist = np.insert(phrase_doc_cos_dist, len(phrase_doc_cos_dist), values=dist, axis=0)\n",
    "        \n",
    "        docs_index = heapq.nlargest(min(top_tfidf,np.count_nonzero(phrase_doc_cos_dist)), range(len(phrase_doc_cos_dist)), phrase_doc_cos_dist.take)\n",
    "#         for index in docs_index:\n",
    "#             print (index,phrase_doc_cos_dist[index])\n",
    "        if len(docs_index) != 0:\n",
    "            index = docs_index[-1]\n",
    "            print (index,phrase_doc_cos_dist[index])\n",
    "        else:\n",
    "            print ('No Documents.')\n",
    "#         print (phrase_doc_cos_dist[docs_index[-1]])\n",
    "    \n",
    "        print ('生成第i个聚类下的di%s'%datetime.now())\n",
    "        # 生成第i个聚类下的di\n",
    "        docs_i = ''\n",
    "        for i in range(len(docs_index)):\n",
    "            docs_i += docs[docs_index[i]]\n",
    "            docs_i += '$$$'\n",
    "        docs_i = docs_i.strip()\n",
    "        docs_cluster.append((label,docs_i))\n",
    "        print ('-'*20)\n",
    "        \n",
    "    print ('Write documents_clustering File......')\n",
    "    # 将list转成df然后存下来\n",
    "    df_labels = ['label','docs']\n",
    "    df_dc = pd.DataFrame.from_records(docs_cluster,columns=df_labels)\n",
    "    df_dc.to_csv('dataset/%s/%s/%s/temp/documents_clustering_1000.csv'%(dataset,model_name,level),index=False)\n",
    "    print ('Completed')\n",
    "    return df_dc.iloc[:,1]\n",
    "    \n",
    "    \n",
    "def adaptive_spherical_kmeans(phrases, docs, threshold=0.5, k=10):\n",
    "    general_topics = pd.DataFrame(columns=['label','phrase','rep'])\n",
    "    sub_topics = pd.DataFrame(columns=['label','phrase','rep'])\n",
    "    k = docs.shape[0]\n",
    "    for i in range(k):\n",
    "        print ('Label: %s'%i)\n",
    "        phrases_i = phrases.loc[phrases['label']==i]['phrase']\n",
    "        for j in range(phrases_i.shape[0]):\n",
    "            phrase = phrases_i.iloc[j]\n",
    "            rep = representativeness(phrase,i,docs)\n",
    "            phrase_rep = {'label': i,'phrase': phrase,'rep': rep}\n",
    "            if j == 0 or j == phrases_i.shape[0]-1:\n",
    "                print (phrase_rep)\n",
    "            if rep < threshold:\n",
    "                general_topics = general_topics.append(phrase_rep, ignore_index=True)\n",
    "            else:\n",
    "                sub_topics = sub_topics.append(phrase_rep, ignore_index=True)\n",
    "        print ('='*30)\n",
    "    return general_topics, sub_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算某个词出现在docs中的次数\n",
    "def tf(docs_k, term=None):\n",
    "    if term == None:\n",
    "        return len(docs_k.split(' '))\n",
    "    else:\n",
    "#         print ('出现次数: %s'%docs_k.count(term))\n",
    "        return docs_k.count(term)\n",
    "\n",
    "# BM25相似\n",
    "def rel(term, k, docs):\n",
    "    k1 = 1\n",
    "    b = 0.75\n",
    "    docs_all = ''\n",
    "    for doc in docs:\n",
    "        docs_all += doc.strip()\n",
    "    idf = math.log((len(docs_all.split(' '))-docs_all.count(term)+0.5)/(docs_all.count(term)+0.5))\n",
    "    R = (docs[k].strip().count(term)*(k1+1))/\\\n",
    "        (docs[k].strip().count(term)+k1*(1-b+b*len(docs[k].strip().split(' ')))/(len(docs_all.split(' '))/len(docs)))\n",
    "#     print ('doc_label:%s\\nidf:%s\\nR:%s'%(k,idf,R))\n",
    "#     print ('='*20)\n",
    "    return idf*R\n",
    "    \n",
    "# 计算某个词在docs集合中的popularity，即在docs_k中大量出现，越小表示越general\n",
    "def popularity(term, k, docs):\n",
    "    pop = math.log(tf(docs[k], term)+1)/math.log(tf(docs[k])+1)\n",
    "#     print ('popularity: %s'%pop)\n",
    "    return pop\n",
    "  \n",
    "# 计算某个词在docs集合中的concentration，即只在这个docs_k中出现而很少出现在docs_k-，越小表示越general\n",
    "def concentration(term, k, docs):\n",
    "    den = 1.0\n",
    "    for i in range(len(docs)):\n",
    "        den += math.exp(rel(term, i, docs))\n",
    "    con = math.exp(rel(term, k, docs)/den)\n",
    "#     print ('concentration: %s'%con)\n",
    "    return con\n",
    "\n",
    "# 计算某个词在docs集合中的representativeness，如果越高表示越属于这个cluster，反之为general的词                  \n",
    "def representativeness(term, k, docs):\n",
    "    rep = math.sqrt(popularity(term, k, docs)*concentration(term, k, docs))\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_tree_generator(phrases, docs, threshold=0.5, k=10, phrases_labels=None, docs_labels=None):\n",
    "    \n",
    "    # 输入话题集合，文本集合；\n",
    "    # 输出更新的话题向量\n",
    "    start_time = datetime.now()\n",
    "    print ('Start local embedding process.\\nStart time: %s'%start_time)\n",
    "    phrases_vector = local_embedding(phrases, docs)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入更新的话题向量\n",
    "    # 输出贴有label的聚完k类的话题集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start spherical kmeans process.\\nStart time: %s'%start_time)\n",
    "    phrases_labels = spherical_kmeans(phrases_vector, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入k类话题集合和全体文本集合\n",
    "    # 输出贴有label的聚完k类的文本集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start extract docs by phrases process.\\nStart time: %s'%start_time)\n",
    "    docs_labels = extract_docs_by_phrases(phrases_labels, docs, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    # sys.exit()\n",
    "    \n",
    "    # 输入k类话题集合和k类文本集合\n",
    "    # 输出常见话题和k类子话题集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start adaptive spherical k means process.\\nStart time: %s'%start_time)\n",
    "    general_topics, sub_topics = adaptive_spherical_kmeans(phrases_labels, docs_labels, threshold, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "\n",
    "    # 返回常见话题和k类子话题集合\n",
    "    return general_topics, sub_topics\n",
    "\n",
    "\n",
    "def topic_tree_generator_kmeans(phrases, docs, threshold=0.5, k=10, phrases_labels=None, docs_labels=None):\n",
    "    \n",
    "    # 输入话题集合，文本集合；\n",
    "    # 输出更新的话题向量\n",
    "    start_time = datetime.now()\n",
    "    print ('Start local embedding process.\\nStart time: %s'%start_time)\n",
    "    phrases_vector = local_embedding(phrases, docs)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入更新的话题向量\n",
    "    # 输出贴有label的聚完k类的话题集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start kmeans process.\\nStart time: %s'%start_time)\n",
    "    phrases_labels = kmeans(phrases_vector, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入k类话题集合和全体文本集合\n",
    "    # 输出贴有label的聚完k类的文本集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start extract docs by phrases process.\\nStart time: %s'%start_time)\n",
    "    docs_labels = extract_docs_by_phrases(phrases_labels, docs, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    # sys.exit()\n",
    "    \n",
    "    # 输入k类话题集合和k类文本集合\n",
    "    # 输出常见话题和k类子话题集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start adaptive spherical k means process.\\nStart time: %s'%start_time)\n",
    "    general_topics, sub_topics = adaptive_spherical_kmeans(phrases_labels, docs_labels, threshold, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "\n",
    "    # 返回常见话题和k类子话题集合\n",
    "    return general_topics, sub_topics\n",
    "\n",
    "\n",
    "def topic_tree_generator_noac(phrases, docs, threshold=0.5, k=10, phrases_labels=None, docs_labels=None):\n",
    "    # 不会啊...怎么分层啊。。。\n",
    "    # 那就不分层，只做分类\n",
    "    \n",
    "    # 输入话题集合，文本集合；\n",
    "    # 输出更新的话题向量\n",
    "    start_time = datetime.now()\n",
    "    print ('Start local embedding process.\\nStart time: %s'%start_time)\n",
    "    phrases_vector = local_embedding(phrases, docs)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入更新的话题向量\n",
    "    # 输出贴有label的聚完k类的话题集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start spherical kmeans process.\\nStart time: %s'%start_time)\n",
    "    phrases_labels = spherical_kmeans(phrases_vector, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    \n",
    "    # 输入k类话题集合和全体文本集合\n",
    "    # 输出贴有label的聚完k类的文本集合\n",
    "    start_time = datetime.now()\n",
    "    print ('Start extract docs by phrases process.\\nStart time: %s'%start_time)\n",
    "    docs_labels = extract_docs_by_phrases(phrases_labels, docs, k)\n",
    "    end_time = datetime.now()\n",
    "    print ('Time end: %s'%(end_time))\n",
    "    print ('Time costed: %s'%(end_time-start_time))\n",
    "    # sys.exit()\n",
    "    \n",
    "    return phrases_labels,phrases_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ieee\\ vision\\ dblp\n",
    "dataset = 'ieee'\n",
    "# top keywords\n",
    "top = 1000\n",
    "# word2vec size \n",
    "size = 128\n",
    "# num of clustering\n",
    "k = 5\n",
    "# top tf-idf documents\n",
    "top_tfidf = 1000\n",
    "# threshold of representitiveness\n",
    "threshold = 0.4\n",
    "# choice of model TTG/TTG_k/NoAC/NoLE\n",
    "model_name = 'TTG'\n",
    "# level\n",
    "level = '_0_0_k6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "      <th>rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>road_traffic</td>\n",
       "      <td>0.524361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>visual_analytics</td>\n",
       "      <td>0.528118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>video_surveillance</td>\n",
       "      <td>0.531690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>risk_assessment</td>\n",
       "      <td>0.541453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>smart_buildings</td>\n",
       "      <td>0.552684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>cyber_physical_systems</td>\n",
       "      <td>0.555233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>wearable_devices</td>\n",
       "      <td>0.555233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>requirements_engineering</td>\n",
       "      <td>0.562366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>intrusion_detection</td>\n",
       "      <td>0.564589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>software_architecture</td>\n",
       "      <td>0.570860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>smart_grids</td>\n",
       "      <td>0.581902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>user_interface</td>\n",
       "      <td>0.581903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>energy_management</td>\n",
       "      <td>0.581902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>environmental_monitoring</td>\n",
       "      <td>0.583580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>real_world</td>\n",
       "      <td>0.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>engineering_education</td>\n",
       "      <td>0.595675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>public_transport</td>\n",
       "      <td>0.611568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>natural_language</td>\n",
       "      <td>0.527988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>life_cycle</td>\n",
       "      <td>0.531770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>cyber_attacks</td>\n",
       "      <td>0.551077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>android_application</td>\n",
       "      <td>0.570663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>supply_chain</td>\n",
       "      <td>0.574808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>cloud_storage</td>\n",
       "      <td>0.578718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>augmented_reality</td>\n",
       "      <td>0.582418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>mobile_phone</td>\n",
       "      <td>0.590873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>user_experience</td>\n",
       "      <td>0.595482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>social_networks</td>\n",
       "      <td>0.608889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>business_processes</td>\n",
       "      <td>0.615799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>semantic_web</td>\n",
       "      <td>0.622048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>waste_management</td>\n",
       "      <td>0.644797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>mobile_crowd_sensing</td>\n",
       "      <td>0.645799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>network_traffic</td>\n",
       "      <td>0.560464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>privacy_preserving</td>\n",
       "      <td>0.574616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>fraud_detection</td>\n",
       "      <td>0.580812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>case_studies</td>\n",
       "      <td>0.583729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>0.603673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>project_management</td>\n",
       "      <td>0.621061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                    phrase       rep\n",
       "0       0              road_traffic  0.524361\n",
       "1       0          visual_analytics  0.528118\n",
       "2       0        video_surveillance  0.531690\n",
       "3       0           risk_assessment  0.541453\n",
       "4       0           smart_buildings  0.552684\n",
       "5       0    cyber_physical_systems  0.555233\n",
       "6       0          wearable_devices  0.555233\n",
       "7       0  requirements_engineering  0.562366\n",
       "8       0       intrusion_detection  0.564589\n",
       "9       0     software_architecture  0.570860\n",
       "10      0               smart_grids  0.581902\n",
       "11      0            user_interface  0.581903\n",
       "12      0         energy_management  0.581902\n",
       "13      0  environmental_monitoring  0.583580\n",
       "14      0                real_world  0.589900\n",
       "15      0     engineering_education  0.595675\n",
       "16      0          public_transport  0.611568\n",
       "17      1          natural_language  0.527988\n",
       "18      1                life_cycle  0.531770\n",
       "19      1             cyber_attacks  0.551077\n",
       "20      1       android_application  0.570663\n",
       "21      1              supply_chain  0.574808\n",
       "22      1             cloud_storage  0.578718\n",
       "23      1         augmented_reality  0.582418\n",
       "24      1              mobile_phone  0.590873\n",
       "25      1           user_experience  0.595482\n",
       "26      1           social_networks  0.608889\n",
       "27      1        business_processes  0.615799\n",
       "28      1              semantic_web  0.622048\n",
       "29      2          waste_management  0.644797\n",
       "30      3      mobile_crowd_sensing  0.645799\n",
       "31      4           network_traffic  0.560464\n",
       "32      4        privacy_preserving  0.574616\n",
       "33      4           fraud_detection  0.580812\n",
       "34      4              case_studies  0.583729\n",
       "35      4           virtual_reality  0.603673\n",
       "36      4        project_management  0.621061"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = pd.read_csv('/Users/wh/Desktop/ieee_remote_k6/TTG/_0/sub_topics.csv')\n",
    "# a = pd.read_csv('dataset/ieee/TTG/_0_1_k6/sub_topics.csv')\n",
    "a = pd.read_csv('dataset/ieee/TTG/_0_0_2_k6/sub_topics.csv')\n",
    "# a.loc[a['label']==5].sort_values(['label','rep'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_0_0_0_k6\n",
      "0 2018-05-23 21:43:16.457142\n",
      "Start local embedding process.\n",
      "Start time: 2018-05-23 21:43:16.466885\n",
      "Docs not exists, need to generate docs first.\n",
      "Train model\n",
      "Time end: 2018-05-23 21:43:16.809360\n",
      "Time costed: 0:00:00.342475\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 21:43:16.809521\n",
      "Write phrases_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:16.878856\n",
      "Time costed: 0:00:00.069335\n",
      "Start extract docs by phrases process.\n",
      "Start time: 2018-05-23 21:43:16.878885\n",
      "Label: 0\n",
      "special_session\n",
      "(1, 301) [0.26736199]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:16.919648\n",
      "计算文档向量2018-05-23 21:43:16.919739\n",
      "294 1.000000015421169\n",
      "生成第i个聚类下的di2018-05-23 21:43:16.934952\n",
      "--------------------\n",
      "Label: 1\n",
      "visually_impaired\n",
      "(1, 301) [0.12832971]\n",
      "cyber_physical\n",
      "(1, 301) [0.92089647]\n",
      "Num of phrases: 29\n",
      "计算平均词向量2018-05-23 21:43:17.051108\n",
      "计算文档向量2018-05-23 21:43:17.052107\n",
      "279 0.9944869426137765\n",
      "生成第i个聚类下的di2018-05-23 21:43:17.141942\n",
      "--------------------\n",
      "Label: 2\n",
      "keynote_speech\n",
      "(1, 301) [0.14431885]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:17.148735\n",
      "计算文档向量2018-05-23 21:43:17.148915\n",
      "287 1.0000000251807846\n",
      "生成第i个聚类下的di2018-05-23 21:43:17.168791\n",
      "--------------------\n",
      "Label: 3\n",
      "ddos_attacks\n",
      "(1, 301) [0.15758775]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:17.173929\n",
      "计算文档向量2018-05-23 21:43:17.174026\n",
      "176 0.9999999956894826\n",
      "生成第i个聚类下的di2018-05-23 21:43:17.196218\n",
      "--------------------\n",
      "Label: 4\n",
      "preliminary_results\n",
      "(1, 301) [0.12816734]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:17.201279\n",
      "计算文档向量2018-05-23 21:43:17.201422\n",
      "298 0.9999999908917883\n",
      "生成第i个聚类下的di2018-05-23 21:43:17.221414\n",
      "--------------------\n",
      "Write documents_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:17.232468\n",
      "Time costed: 0:00:00.353583\n",
      "Start adaptive spherical k means process.\n",
      "Start time: 2018-05-23 21:43:17.232556\n",
      "Label: 0\n",
      "{'label': 0, 'phrase': 'special_session', 'rep': 0.5361799581200278}\n",
      "==============================\n",
      "Label: 1\n",
      "{'label': 1, 'phrase': 'visually_impaired', 'rep': 0.40823800690857015}\n",
      "{'label': 1, 'phrase': 'cyber_physical', 'rep': 0.5949766924387054}\n",
      "==============================\n",
      "Label: 2\n",
      "{'label': 2, 'phrase': 'keynote_speech', 'rep': 0.5032430970573999}\n",
      "==============================\n",
      "Label: 3\n",
      "{'label': 3, 'phrase': 'ddos_attacks', 'rep': 0.526149349775516}\n",
      "==============================\n",
      "Label: 4\n",
      "{'label': 4, 'phrase': 'preliminary_results', 'rep': 0.5170576710330181}\n",
      "==============================\n",
      "Time end: 2018-05-23 21:43:19.347411\n",
      "Time costed: 0:00:02.114855\n",
      "Time costed for TTG model: 0:00:02.880627\n",
      "_0_0_1_k6\n",
      "0 2018-05-23 21:43:19.463524\n",
      "Start local embedding process.\n",
      "Start time: 2018-05-23 21:43:19.493771\n",
      "Docs not exists, need to generate docs first.\n",
      "Train model\n",
      "Time end: 2018-05-23 21:43:20.473712\n",
      "Time costed: 0:00:00.979941\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 21:43:20.473932\n",
      "Write phrases_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:20.529131\n",
      "Time costed: 0:00:00.055199\n",
      "Start extract docs by phrases process.\n",
      "Start time: 2018-05-23 21:43:20.529171\n",
      "Label: 0\n",
      "health_care\n",
      "(1, 1001) [1.98547514]\n",
      "data_analytics\n",
      "(1, 1001) [4.10067008]\n",
      "Num of phrases: 2\n",
      "计算平均词向量2018-05-23 21:43:20.654820\n",
      "计算文档向量2018-05-23 21:43:20.655335\n",
      "68 0.9999259474703709\n",
      "生成第i个聚类下的di2018-05-23 21:43:20.755195\n",
      "--------------------\n",
      "Label: 1\n",
      "higher_education\n",
      "(1, 1001) [2.50765392]\n",
      "decision_making\n",
      "(1, 1001) [2.23544836]\n",
      "Num of phrases: 2\n",
      "计算平均词向量2018-05-23 21:43:20.790680\n",
      "计算文档向量2018-05-23 21:43:20.790811\n",
      "762 0.9999390082782205\n",
      "生成第i个聚类下的di2018-05-23 21:43:20.863086\n",
      "--------------------\n",
      "Label: 2\n",
      "mobile_apps\n",
      "(1, 1001) [2.23825245]\n",
      "data_mining\n",
      "(1, 1001) [3.12783503]\n",
      "Num of phrases: 2\n",
      "计算平均词向量2018-05-23 21:43:20.891290\n",
      "计算文档向量2018-05-23 21:43:20.891538\n",
      "120 0.9999586881139123\n",
      "生成第i个聚类下的di2018-05-23 21:43:20.967843\n",
      "--------------------\n",
      "Label: 3\n",
      "social_media\n",
      "(1, 1001) [3.57764509]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:20.984678\n",
      "计算文档向量2018-05-23 21:43:20.984904\n",
      "966 1.000000000653858\n",
      "生成第i个聚类下的di2018-05-23 21:43:21.037318\n",
      "--------------------\n",
      "Label: 4\n",
      "big_data_analytics\n",
      "(1, 1001) [3.30135832]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:21.051354\n",
      "计算文档向量2018-05-23 21:43:21.051454\n",
      "544 1.0000000284428237\n",
      "生成第i个聚类下的di2018-05-23 21:43:21.102873\n",
      "--------------------\n",
      "Write documents_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:21.154837\n",
      "Time costed: 0:00:00.625666\n",
      "Start adaptive spherical k means process.\n",
      "Start time: 2018-05-23 21:43:21.155133\n",
      "Label: 0\n",
      "{'label': 0, 'phrase': 'health_care', 'rep': 0.629400260665161}\n",
      "{'label': 0, 'phrase': 'data_analytics', 'rep': 0.7562609010725073}\n",
      "==============================\n",
      "Label: 1\n",
      "{'label': 1, 'phrase': 'higher_education', 'rep': 0.6921418140227368}\n",
      "{'label': 1, 'phrase': 'decision_making', 'rep': 0.7051199369397695}\n",
      "==============================\n",
      "Label: 2\n",
      "{'label': 2, 'phrase': 'mobile_apps', 'rep': 0.6877511084008334}\n",
      "{'label': 2, 'phrase': 'data_mining', 'rep': 0.7107714120368008}\n",
      "==============================\n",
      "Label: 3\n",
      "{'label': 3, 'phrase': 'social_media', 'rep': 0.7463788573890936}\n",
      "==============================\n",
      "Label: 4\n",
      "{'label': 4, 'phrase': 'big_data_analytics', 'rep': 0.7373027508487895}\n",
      "==============================\n",
      "Time end: 2018-05-23 21:43:23.362442\n",
      "Time costed: 0:00:02.207309\n",
      "Time costed for TTG model: 0:00:03.868779\n",
      "_0_0_2_k6\n",
      "0 2018-05-23 21:43:23.435178\n",
      "Start local embedding process.\n",
      "Start time: 2018-05-23 21:43:23.467617\n",
      "Docs not exists, need to generate docs first.\n",
      "Train model\n",
      "Time end: 2018-05-23 21:43:24.379363\n",
      "Time costed: 0:00:00.911746\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 21:43:24.379497\n",
      "Write phrases_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:24.479409\n",
      "Time costed: 0:00:00.099912\n",
      "Start extract docs by phrases process.\n",
      "Start time: 2018-05-23 21:43:24.479450\n",
      "Label: 0\n",
      "road_traffic\n",
      "(1, 846) [0.6521372]\n",
      "public_transport\n",
      "(1, 846) [1.52369308]\n",
      "Num of phrases: 17\n",
      "计算平均词向量2018-05-23 21:43:24.767781\n",
      "计算文档向量2018-05-23 21:43:24.768332\n",
      "194 0.9996788753546304\n",
      "生成第i个聚类下的di2018-05-23 21:43:24.927447\n",
      "--------------------\n",
      "Label: 1\n",
      "natural_language\n",
      "(1, 846) [0.60866973]\n",
      "semantic_web\n",
      "(1, 846) [1.46149476]\n",
      "Num of phrases: 12\n",
      "计算平均词向量2018-05-23 21:43:25.075140\n",
      "计算文档向量2018-05-23 21:43:25.075636\n",
      "177 0.9996611104675084\n",
      "生成第i个聚类下的di2018-05-23 21:43:25.224029\n",
      "--------------------\n",
      "Label: 2\n",
      "waste_management\n",
      "(1, 846) [0.65920795]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:25.241472\n",
      "计算文档向量2018-05-23 21:43:25.241617\n",
      "299 1.0000000281293748\n",
      "生成第i个聚类下的di2018-05-23 21:43:25.289592\n",
      "--------------------\n",
      "Label: 3\n",
      "mobile_crowd_sensing\n",
      "(1, 846) [0.58027717]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:25.312592\n",
      "计算文档向量2018-05-23 21:43:25.312753\n",
      "381 1.0000000235149102\n",
      "生成第i个聚类下的di2018-05-23 21:43:25.375799\n",
      "--------------------\n",
      "Label: 4\n",
      "network_traffic\n",
      "(1, 846) [0.4893153]\n",
      "project_management\n",
      "(1, 846) [1.01728589]\n",
      "Num of phrases: 6\n",
      "计算平均词向量2018-05-23 21:43:25.533500\n",
      "计算文档向量2018-05-23 21:43:25.533830\n",
      "29 0.9996668911280259\n",
      "生成第i个聚类下的di2018-05-23 21:43:25.669540\n",
      "--------------------\n",
      "Write documents_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:25.705365\n",
      "Time costed: 0:00:01.225915\n",
      "Start adaptive spherical k means process.\n",
      "Start time: 2018-05-23 21:43:25.706123\n",
      "Label: 0\n",
      "{'label': 0, 'phrase': 'road_traffic', 'rep': 0.5243607226034561}\n",
      "{'label': 0, 'phrase': 'public_transport', 'rep': 0.611568354665879}\n",
      "==============================\n",
      "Label: 1\n",
      "{'label': 1, 'phrase': 'natural_language', 'rep': 0.527987520633128}\n",
      "{'label': 1, 'phrase': 'semantic_web', 'rep': 0.6220478346023238}\n",
      "==============================\n",
      "Label: 2\n",
      "{'label': 2, 'phrase': 'waste_management', 'rep': 0.6447973727156577}\n",
      "==============================\n",
      "Label: 3\n",
      "{'label': 3, 'phrase': 'mobile_crowd_sensing', 'rep': 0.6457987135214666}\n",
      "==============================\n",
      "Label: 4\n",
      "{'label': 4, 'phrase': 'network_traffic', 'rep': 0.560464241953953}\n",
      "{'label': 4, 'phrase': 'project_management', 'rep': 0.6210605108812458}\n",
      "==============================\n",
      "Time end: 2018-05-23 21:43:32.453694\n",
      "Time costed: 0:00:06.747571\n",
      "Time costed for TTG model: 0:00:08.986158\n",
      "_0_0_3_k6\n",
      "0 2018-05-23 21:43:32.514515\n",
      "Start local embedding process.\n",
      "Start time: 2018-05-23 21:43:32.544258\n",
      "Docs not exists, need to generate docs first.\n",
      "Train model\n",
      "Time end: 2018-05-23 21:43:33.350567\n",
      "Time costed: 0:00:00.806309\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 21:43:33.350713\n",
      "Write phrases_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:33.383220\n",
      "Time costed: 0:00:00.032507\n",
      "Start extract docs by phrases process.\n",
      "Start time: 2018-05-23 21:43:33.383259\n",
      "Label: 0\n",
      "cloud_computing\n",
      "(1, 1001) [4.135257]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:33.477056\n",
      "计算文档向量2018-05-23 21:43:33.477193\n",
      "566 0.9999999815062763\n",
      "生成第i个聚类下的di2018-05-23 21:43:33.552072\n",
      "--------------------\n",
      "Label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_home\n",
      "(1, 1001) [3.40652492]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:33.569739\n",
      "计算文档向量2018-05-23 21:43:33.569841\n",
      "999 1.0000000201184331\n",
      "生成第i个聚类下的di2018-05-23 21:43:33.630899\n",
      "--------------------\n",
      "Label: 2\n",
      "big_data\n",
      "(1, 1001) [5.36102823]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:33.653166\n",
      "计算文档向量2018-05-23 21:43:33.653527\n",
      "912 1.0000000155240822\n",
      "生成第i个聚类下的di2018-05-23 21:43:33.721134\n",
      "--------------------\n",
      "Label: 3\n",
      "smart_city\n",
      "(1, 1001) [4.80256074]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:33.736573\n",
      "计算文档向量2018-05-23 21:43:33.736672\n",
      "647 1.0000000129133375\n",
      "生成第i个聚类下的di2018-05-23 21:43:33.809042\n",
      "--------------------\n",
      "Label: 4\n",
      "smart_cities\n",
      "(1, 1001) [4.01063444]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:33.825538\n",
      "计算文档向量2018-05-23 21:43:33.825683\n",
      "905 0.9999999933645225\n",
      "生成第i个聚类下的di2018-05-23 21:43:33.924951\n",
      "--------------------\n",
      "Write documents_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:33.967013\n",
      "Time costed: 0:00:00.583754\n",
      "Start adaptive spherical k means process.\n",
      "Start time: 2018-05-23 21:43:33.967119\n",
      "Label: 0\n",
      "{'label': 0, 'phrase': 'cloud_computing', 'rep': 0.7585490180812854}\n",
      "==============================\n",
      "Label: 1\n",
      "{'label': 1, 'phrase': 'smart_home', 'rep': 0.7434057753839601}\n",
      "==============================\n",
      "Label: 2\n",
      "{'label': 2, 'phrase': 'big_data', 'rep': 0.758512022006904}\n",
      "==============================\n",
      "Label: 3\n",
      "{'label': 3, 'phrase': 'smart_city', 'rep': 0.7646590543255671}\n",
      "==============================\n",
      "Label: 4\n",
      "{'label': 4, 'phrase': 'smart_cities', 'rep': 0.7543626980667718}\n",
      "==============================\n",
      "Time end: 2018-05-23 21:43:35.473405\n",
      "Time costed: 0:00:01.506286\n",
      "Time costed for TTG model: 0:00:02.929341\n",
      "_0_0_4_k6\n",
      "0 2018-05-23 21:43:35.588310\n",
      "Start local embedding process.\n",
      "Start time: 2018-05-23 21:43:35.659677\n",
      "Docs not exists, need to generate docs first.\n",
      "Train model\n",
      "Time end: 2018-05-23 21:43:36.686284\n",
      "Time costed: 0:00:01.026607\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 21:43:36.686432\n",
      "Write phrases_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:36.758569\n",
      "Time costed: 0:00:00.072137\n",
      "Start extract docs by phrases process.\n",
      "Start time: 2018-05-23 21:43:36.758597\n",
      "Label: 0\n",
      "risk_management\n",
      "(1, 1001) [1.7406988]\n",
      "case_study\n",
      "(1, 1001) [0.89254715]\n",
      "Num of phrases: 3\n",
      "计算平均词向量2018-05-23 21:43:36.872389\n",
      "计算文档向量2018-05-23 21:43:36.872530\n",
      "471 0.9997405657591839\n",
      "生成第i个聚类下的di2018-05-23 21:43:36.953579\n",
      "--------------------\n",
      "Label: 1\n",
      "predictive_analytics\n",
      "(1, 1001) [1.40756072]\n",
      "mobile_app\n",
      "(1, 1001) [4.12610342]\n",
      "Num of phrases: 9\n",
      "计算平均词向量2018-05-23 21:43:37.066531\n",
      "计算文档向量2018-05-23 21:43:37.066926\n",
      "579 0.9998609576046055\n",
      "生成第i个聚类下的di2018-05-23 21:43:37.207458\n",
      "--------------------\n",
      "Label: 2\n",
      "quality_assurance\n",
      "(1, 1001) [1.32248171]\n",
      "decision_support\n",
      "(1, 1001) [1.13808606]\n",
      "Num of phrases: 7\n",
      "计算平均词向量2018-05-23 21:43:37.305055\n",
      "计算文档向量2018-05-23 21:43:37.305277\n",
      "868 0.9997748528175643\n",
      "生成第i个聚类下的di2018-05-23 21:43:37.419790\n",
      "--------------------\n",
      "Label: 3\n",
      "social_networking\n",
      "(1, 1001) [1.63447117]\n",
      "data_collection\n",
      "(1, 1001) [1.16255957]\n",
      "Num of phrases: 5\n",
      "计算平均词向量2018-05-23 21:43:37.498719\n",
      "计算文档向量2018-05-23 21:43:37.499849\n",
      "979 0.9998295956608865\n",
      "生成第i个聚类下的di2018-05-23 21:43:37.598744\n",
      "--------------------\n",
      "Label: 4\n",
      "apache_spark\n",
      "(1, 1001) [0.37778083]\n",
      "Num of phrases: 1\n",
      "计算平均词向量2018-05-23 21:43:37.613288\n",
      "计算文档向量2018-05-23 21:43:37.613425\n",
      "980 1.00000001790026\n",
      "生成第i个聚类下的di2018-05-23 21:43:37.664614\n",
      "--------------------\n",
      "Write documents_clustering File......\n",
      "Completed\n",
      "Time end: 2018-05-23 21:43:37.722764\n",
      "Time costed: 0:00:00.964167\n",
      "Start adaptive spherical k means process.\n",
      "Start time: 2018-05-23 21:43:37.722846\n",
      "Label: 0\n",
      "{'label': 0, 'phrase': 'risk_management', 'rep': 0.6585194617733743}\n",
      "{'label': 0, 'phrase': 'case_study', 'rep': 0.6308524737472168}\n",
      "==============================\n",
      "Label: 1\n",
      "{'label': 1, 'phrase': 'predictive_analytics', 'rep': 0.5951423789283901}\n",
      "{'label': 1, 'phrase': 'mobile_app', 'rep': 0.7206848607474635}\n",
      "==============================\n",
      "Label: 2\n",
      "{'label': 2, 'phrase': 'quality_assurance', 'rep': 0.59217347907573}\n",
      "{'label': 2, 'phrase': 'decision_support', 'rep': 0.5921735411088704}\n",
      "==============================\n",
      "Label: 3\n",
      "{'label': 3, 'phrase': 'social_networking', 'rep': 0.6264356326815008}\n",
      "{'label': 3, 'phrase': 'data_collection', 'rep': 0.6336670888970176}\n",
      "==============================\n",
      "Label: 4\n",
      "{'label': 4, 'phrase': 'apache_spark', 'rep': 0.614783350421244}\n",
      "==============================\n",
      "Time end: 2018-05-23 21:43:44.466445\n",
      "Time costed: 0:00:06.743599\n",
      "Time costed for TTG model: 0:00:08.806877\n"
     ]
    }
   ],
   "source": [
    "for label_i in range(0,5):\n",
    "    # level\n",
    "    model_name = 'TTG'\n",
    "    level = '_0_0_%s_k6'%label_i\n",
    "    print (level)\n",
    "#     phrases_path = '/Users/wh/Desktop/ieee_remote_k6/TTG/_0/sub_topics.csv'\n",
    "    phrases_path = 'dataset/ieee/TTG_k/_0_0_k6/sub_topics.csv'\n",
    "    phrases_file = pd.read_csv(phrases_path)\n",
    "    phrases = phrases_file.loc[phrases_file['label']==label_i]['phrase'].tolist()\n",
    "\n",
    "    #     docs_path = '/Users/wh/Desktop/ieee_remote_k6/TTG/_0/temp/documents_clustering_1000.csv'\n",
    "    docs_path = 'dataset/ieee/TTG_k/_0_0_k6/temp/documents_clustering_1000.csv'\n",
    "    docs_file = pd.read_csv(docs_path)\n",
    "    docs_str = docs_file.loc[docs_file['label']==label_i].iloc[0,1]\n",
    "    docs = replace_all(docs_str.lower(), phrases).split('$$$')\n",
    "    pd.DataFrame(docs, columns=[\"docs\"]).to_csv('dataset/%s/%s/docs_after_replace.csv'%(dataset,level),index=False)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    model_choice = {\n",
    "        'TTG':topic_tree_generator,\n",
    "        'NoAC':topic_tree_generator_noac,\n",
    "        'NoLE':topic_tree_generator,\n",
    "        'TTG_k':topic_tree_generator_kmeans\n",
    "    }\n",
    "\n",
    "    general_topics, sub_topics = model_choice[model_name](phrases, docs, threshold, k)\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    print ('Time costed for %s model: %s'%(model_name,end_time-start_time))\n",
    "\n",
    "    general_topics.to_csv('dataset/%s/%s/%s/general_topics.csv'%(dataset,model_name,level),index=False)\n",
    "    sub_topics.to_csv('dataset/%s/%s/%s/sub_topics.csv'%(dataset,model_name,level),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 测试ieee的sub_topics\n",
    "phrases_data = pd.read_csv('dataset/%s/%s/sub_topics.csv'%(dataset,level),sep=',')\n",
    "phrases = phrases_data.loc[phrases_data['label']==1]['phrase'].tolist()\n",
    "docs_str = pd.read_csv('dataset/%s/%s/temp/documents_clustering.csv'%(dataset,level),sep=',').iloc[1,1]\n",
    "docs = replace_all(docs_str.lower(), phrases).split('$$$')\n",
    "'''\n",
    "\n",
    "# 测试ieee根节点TOP5000的topics\n",
    "start_time = datetime.now()\n",
    "# phrases = pd.read_csv('dataset/%s/%s/nps_autophrase.csv'%(dataset,level),sep='\\t').iloc[:top,1].tolist()\n",
    "if model_name == 'NoLE':\n",
    "    docs_path = 'dataset/%s/_0/docs_after_replace.csv'%(dataset)\n",
    "else:\n",
    "    docs_path = 'dataset/%s/%s/docs_after_replace.csv'%(dataset,level)\n",
    "if os.path.isfile(docs_path):\n",
    "    docs = pd.read_csv(docs_path)['docs'].tolist()\n",
    "else:\n",
    "    docs_data = pd.read_csv('dataset/%s/%s/papers.csv'%(dataset,level),sep='\\t')\n",
    "    docs_str = ''\n",
    "    for i in range(docs_data.shape[0]): \n",
    "        if dataset == 'dblp':\n",
    "            doc_str = str(docs_data.iloc[i,0])+'. $$$ '\n",
    "            docs_str += doc_str\n",
    "        else:\n",
    "            doc_str = str(docs_data.iloc[i,1])+'. ### '+str(docs_data.iloc[i,2])+' $$$ '\n",
    "            docs_str += doc_str\n",
    "    print ('Start replace phrases in documents.')\n",
    "    docs = replace_all(docs_str.lower(), phrases).split('$$$')\n",
    "    pd.DataFrame(docs, columns=[\"docs\"]).to_csv('dataset/%s/%s/docs_after_replace.csv'%(dataset,level),index=False)\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    print ('Time costed for collection: %s'%(end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start local embedding process.\n",
      "Start time: 2018-05-23 16:37:31.888615\n",
      "Load model\n",
      "Time end: 2018-05-23 16:37:34.914292\n",
      "Time costed: 0:00:03.025677\n",
      "Start spherical kmeans process.\n",
      "Start time: 2018-05-23 16:37:34.914390\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-408-ac77ba547d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgeneral_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_choice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fa3f5d9593d2>\u001b[0m in \u001b[0;36mtopic_tree_generator\u001b[0;34m(phrases, docs, threshold, k, phrases_labels, docs_labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Start spherical kmeans process.\\nStart time: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mphrases_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspherical_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Time end: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b3f385f5ab82>\u001b[0m in \u001b[0;36mspherical_kmeans\u001b[0;34m(phrases_vector, k)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mskm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSphericalKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Fitting the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Getting the cluster labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/spherecluster/spherical_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1412\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "model_choice = {\n",
    "    'TTG':topic_tree_generator,\n",
    "    'NoAC':topic_tree_generator_noac,\n",
    "    'NoLE':topic_tree_generator,\n",
    "    'TTG_k':topic_tree_generator_kmeans\n",
    "}\n",
    "\n",
    "general_topics, sub_topics = model_choice[model_name](phrases, docs, threshold, k)\n",
    "end_time = datetime.now()\n",
    "\n",
    "print ('Time costed for %s model: %s'%(model_name,end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_topics.to_csv('dataset/%s/%s/%s/general_topics.csv'%(dataset,model_name,level),index=False)\n",
    "sub_topics.to_csv('dataset/%s/%s/%s/sub_topics.csv'%(dataset,model_name,level),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "      <th>rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>asynchronous_motor</td>\n",
       "      <td>0.387404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>electromagnetic_interference</td>\n",
       "      <td>0.456226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>modular_multilevel_converter</td>\n",
       "      <td>0.430138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>inductive_power_transfer</td>\n",
       "      <td>0.404448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>switched_capacitor</td>\n",
       "      <td>0.448969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>phase_locked_loop</td>\n",
       "      <td>0.418646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>equivalent_circuit</td>\n",
       "      <td>0.464958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>wind_generator</td>\n",
       "      <td>0.464958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>dual_active_bridge</td>\n",
       "      <td>0.471627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>pfc_converter</td>\n",
       "      <td>0.471627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>fault_tolerant</td>\n",
       "      <td>0.483411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>single_stage</td>\n",
       "      <td>0.493572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                        phrase       rep\n",
       "0      0            asynchronous_motor  0.387404\n",
       "1      0  electromagnetic_interference  0.456226\n",
       "2      2  modular_multilevel_converter  0.430138\n",
       "3      3      inductive_power_transfer  0.404448\n",
       "4      3            switched_capacitor  0.448969\n",
       "11     4             phase_locked_loop  0.418646\n",
       "8      4            equivalent_circuit  0.464958\n",
       "9      4                wind_generator  0.464958\n",
       "10     4            dual_active_bridge  0.471627\n",
       "5      4                 pfc_converter  0.471627\n",
       "6      4                fault_tolerant  0.483411\n",
       "7      4                  single_stage  0.493572"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_topics.sort_values(['label','rep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "      <th>rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>synchronous_machines</td>\n",
       "      <td>0.500929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>transient_stability</td>\n",
       "      <td>0.519250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>solar_power</td>\n",
       "      <td>0.522404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>transmission_line</td>\n",
       "      <td>0.528312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>dc_microgrids</td>\n",
       "      <td>0.528312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>control_strategies</td>\n",
       "      <td>0.531087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>gate_driver</td>\n",
       "      <td>0.536320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bldc_motor</td>\n",
       "      <td>0.538794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>circuit_breakers</td>\n",
       "      <td>0.541180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>power_plants</td>\n",
       "      <td>0.541180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>soft_switching</td>\n",
       "      <td>0.543486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>dynamic_voltage_restorer</td>\n",
       "      <td>0.547873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>solar_energy</td>\n",
       "      <td>0.547873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>high_power_density</td>\n",
       "      <td>0.547873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>condition_monitoring</td>\n",
       "      <td>0.549963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>electrical_machines</td>\n",
       "      <td>0.549963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>power_modules</td>\n",
       "      <td>0.549963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>frequency_response</td>\n",
       "      <td>0.549963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>battery_storage</td>\n",
       "      <td>0.555868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>sic_mosfet</td>\n",
       "      <td>0.561291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>electric_drives</td>\n",
       "      <td>0.561291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>distributed_energy_resources</td>\n",
       "      <td>0.563004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>voltage_stability</td>\n",
       "      <td>0.563004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>fault_detection</td>\n",
       "      <td>0.569440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>micro_grid</td>\n",
       "      <td>0.572434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>induction_machines</td>\n",
       "      <td>0.573881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>distribution_grid</td>\n",
       "      <td>0.578037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>power_module</td>\n",
       "      <td>0.585621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>high_efficiency</td>\n",
       "      <td>0.586802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>wind_farms</td>\n",
       "      <td>0.592398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4</td>\n",
       "      <td>active_damping</td>\n",
       "      <td>0.547413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>closed_loop</td>\n",
       "      <td>0.547413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>4</td>\n",
       "      <td>fault_diagnosis</td>\n",
       "      <td>0.555681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4</td>\n",
       "      <td>maximum_power_point_tracking</td>\n",
       "      <td>0.559461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4</td>\n",
       "      <td>brushless_dc_motor</td>\n",
       "      <td>0.563037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>boost_converter</td>\n",
       "      <td>0.572731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4</td>\n",
       "      <td>pmsm_drive</td>\n",
       "      <td>0.577087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4</td>\n",
       "      <td>shunt_active_power_filter</td>\n",
       "      <td>0.582473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4</td>\n",
       "      <td>pulse_width_modulation</td>\n",
       "      <td>0.588628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>4</td>\n",
       "      <td>switched_reluctance_motor</td>\n",
       "      <td>0.589791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>4</td>\n",
       "      <td>induction_motor_drives</td>\n",
       "      <td>0.590934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>4</td>\n",
       "      <td>power_plant</td>\n",
       "      <td>0.593158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4</td>\n",
       "      <td>dc_motor</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>4</td>\n",
       "      <td>coupled_inductor</td>\n",
       "      <td>0.599398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>4</td>\n",
       "      <td>sepic_converter</td>\n",
       "      <td>0.599398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>lcl_filter</td>\n",
       "      <td>0.603239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4</td>\n",
       "      <td>permanent_magnet_synchronous_motor</td>\n",
       "      <td>0.608592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>4</td>\n",
       "      <td>direct_torque_control</td>\n",
       "      <td>0.610279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4</td>\n",
       "      <td>multilevel_inverter</td>\n",
       "      <td>0.612726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>4</td>\n",
       "      <td>control_strategy</td>\n",
       "      <td>0.615845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4</td>\n",
       "      <td>electric_drive</td>\n",
       "      <td>0.621645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>buck_converter</td>\n",
       "      <td>0.624351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4</td>\n",
       "      <td>matrix_converter</td>\n",
       "      <td>0.626942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>droop_control</td>\n",
       "      <td>0.636322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>modular_multilevel_converters</td>\n",
       "      <td>0.640514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>4</td>\n",
       "      <td>induction_motor_drive</td>\n",
       "      <td>0.662631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>dc_microgrid</td>\n",
       "      <td>0.695346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>induction_motor</td>\n",
       "      <td>0.700923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>modular_multilevel</td>\n",
       "      <td>0.705011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4</td>\n",
       "      <td>multilevel_converter</td>\n",
       "      <td>0.711069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                              phrase       rep\n",
       "4       0                synchronous_machines  0.500929\n",
       "33      0                 transient_stability  0.519250\n",
       "5       0                         solar_power  0.522404\n",
       "19      0                   transmission_line  0.528312\n",
       "12      0                       dc_microgrids  0.528312\n",
       "7       0                  control_strategies  0.531087\n",
       "9       0                         gate_driver  0.536320\n",
       "3       0                          bldc_motor  0.538794\n",
       "29      0                    circuit_breakers  0.541180\n",
       "40      0                        power_plants  0.541180\n",
       "1       0                      soft_switching  0.543486\n",
       "23      0            dynamic_voltage_restorer  0.547873\n",
       "10      0                        solar_energy  0.547873\n",
       "32      0                  high_power_density  0.547873\n",
       "21      0                condition_monitoring  0.549963\n",
       "13      0                 electrical_machines  0.549963\n",
       "31      0                       power_modules  0.549963\n",
       "16      0                  frequency_response  0.549963\n",
       "36      0                     battery_storage  0.555868\n",
       "27      0                          sic_mosfet  0.561291\n",
       "15      0                     electric_drives  0.561291\n",
       "30      0        distributed_energy_resources  0.563004\n",
       "20      0                   voltage_stability  0.563004\n",
       "28      0                     fault_detection  0.569440\n",
       "8       0                          micro_grid  0.572434\n",
       "14      0                  induction_machines  0.573881\n",
       "26      0                   distribution_grid  0.578037\n",
       "41      0                        power_module  0.585621\n",
       "35      0                     high_efficiency  0.586802\n",
       "42      0                          wind_farms  0.592398\n",
       "..    ...                                 ...       ...\n",
       "104     4                      active_damping  0.547413\n",
       "99      4                         closed_loop  0.547413\n",
       "122     4                     fault_diagnosis  0.555681\n",
       "113     4        maximum_power_point_tracking  0.559461\n",
       "107     4                  brushless_dc_motor  0.563037\n",
       "96      4                     boost_converter  0.572731\n",
       "121     4                          pmsm_drive  0.577087\n",
       "105     4           shunt_active_power_filter  0.582473\n",
       "120     4              pulse_width_modulation  0.588628\n",
       "116     4           switched_reluctance_motor  0.589791\n",
       "112     4              induction_motor_drives  0.590934\n",
       "111     4                         power_plant  0.593158\n",
       "101     4                            dc_motor  0.598400\n",
       "109     4                    coupled_inductor  0.599398\n",
       "115     4                     sepic_converter  0.599398\n",
       "95      4                          lcl_filter  0.603239\n",
       "106     4  permanent_magnet_synchronous_motor  0.608592\n",
       "110     4               direct_torque_control  0.610279\n",
       "117     4                 multilevel_inverter  0.612726\n",
       "123     4                    control_strategy  0.615845\n",
       "127     4                      electric_drive  0.621645\n",
       "97      4                      buck_converter  0.624351\n",
       "124     4                    matrix_converter  0.626942\n",
       "102     4                       droop_control  0.636322\n",
       "98      4       modular_multilevel_converters  0.640514\n",
       "108     4               induction_motor_drive  0.662631\n",
       "103     4                        dc_microgrid  0.695346\n",
       "100     4                     induction_motor  0.700923\n",
       "125     4                  modular_multilevel  0.705011\n",
       "118     4                multilevel_converter  0.711069\n",
       "\n",
       "[128 rows x 3 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_topics.sort_values(['label','rep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "start_time = datetime.now()\n",
    "\n",
    "phrases_labels = pd.read_csv('dataset/dblp/temp/phrases_clustering.csv',sep=',')\n",
    "docs_labels = pd.read_csv('dataset/dblp/temp/documents_clustering.csv',sep=',').iloc[:,1]\n",
    "\n",
    "general_topics, sub_topics = topic_tree_generator(phrases, docs, 0.5, 5 , phrases_labels, docs_labels)\n",
    "end_time = datetime.now()\n",
    "\n",
    "print ('Time costed for TTG model: %s'%(end_time-start_time))\n",
    "\n",
    "# phrases_labels = topic_tree_generator(phrases, docs, 0.4, 5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for model_name in ['TTG','TTG_k','NoLE']:\n",
    "    sub_topics = pd.read_csv('/Users/wh/Desktop/ieee_remote_k6/%s/_0/sub_topics'%model_name)\n",
    "    general_topics = pd.read_csv('/Users/wh/Desktop/ieee_remote_k6/%s/_0/general_topics'%model_name)\n",
    "    all_topics = pd.concat([sub_topics,general_topics])\n",
    "    sub_topics_new = all_topics.loc[all_topics['rep']>=0.22].sort_values(['label','rep'])\n",
    "    general_topics_new = all_topics.loc[all_topics['rep']<0.22].sort_values(['label','rep'])\n",
    "    sub_topics_new.to_csv('/Users/wh/Desktop/ieee_remote_k6/%s/_0/sub_topics.csv'%model_name,index=False)\n",
    "    general_topics_new.to_csv('/Users/wh/Desktop/ieee_remote_k6/%s/_0/general_topics.csv'%model_name,index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
